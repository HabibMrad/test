{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfxfPWS-tk-3"
      },
      "source": [
        "Â© 2021 Zaka AI, Inc. All Rights Reserved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT3_7pSqQtmQ"
      },
      "source": [
        "#Deep Sentiment Analysis\n",
        "\n",
        "**Objective:** The goal from this exercise is to learn how to integrate Deep Learning into Natural Language Processing through Deep Sentiment Analysis.\n",
        "The sections of this colab exercise are:\n",
        "1. Keras Embedding Layer\n",
        "2. Dataset loading\n",
        "3. Data preparation\n",
        "4. Feature extraction using Word Embeddings\n",
        "5. Recurrent Neural Network model\n",
        "6. Plotting training details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLzGR6lhUF9g"
      },
      "source": [
        "# Keras Embedding Layer\n",
        "\n",
        "Before we start with the Sentiment Analysis exercise, let's look at an example of how to use a Keras Embedding layer. \n",
        "\n",
        "In this example, we will build a `Sequential` model with an `Embedding` layer to learn the embeddings of a series of simple documents defined in the docs variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRpZ3TlXUBsZ"
      },
      "source": [
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "# define documents\n",
        "docs = ['Well done!',\n",
        "\t\t'Good work',\n",
        "\t\t'Great effort',\n",
        "\t\t'nice work',\n",
        "\t\t'Excellent!',\n",
        "\t\t'Weak',\n",
        "\t\t'Poor effort!',\n",
        "\t\t'not good',\n",
        "\t\t'poor work',\n",
        "\t\t'Could have done better.']\n",
        "\n",
        "# define class labels\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
        "\n",
        "# prepare tokenizer\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(docs)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "# integer encode the documents\n",
        "encoded_docs = t.texts_to_sequences(docs)\n",
        "print(encoded_docs)\n",
        "\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)\n",
        "\n",
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# summarize the model\n",
        "model.summary()\n",
        "\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Weql_nx1bKZI"
      },
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "Text classification is one of the important tasks of text mining.\n",
        "\n",
        "![alt text](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1535125878/NLTK3_zwbdgg.png)\n",
        "\n",
        "In this notebook, we will perform Sentiment Analysis on IMDB movies reviews. Sentiment Analysis is the art of extracting people's opinion from digital text. We will use a regression model from Scikit-Learn able to predict the sentiment given a movie review. \n",
        "\n",
        "We will use [the IMDB movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/), which consists of 50,000 movies review (50% are positive, 50% are negative).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is the same exercise we saw in the previous lesson but the differences here are:\n",
        "\n",
        "\n",
        "*   We are using **Word Embeddings** for feature extraction instead of Bag-of-Words. This is done by adding an `Embedding` layer as the first layer in the Sequential model.\n",
        "*   We are using a deep **Recurrent Neural Network** for modeling.\n",
        "\n",
        "These changes should allow the model to better understand the dataset and give better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NG9CLMaT0N7"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_HVs_6nS2F2"
      },
      "source": [
        "### 1. Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxDGpou5cAzB"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "\n",
        "# download Punkt Sentence Tokenizer\n",
        "nltk.download('punkt')\n",
        "# download stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5blEsxShTEV"
      },
      "source": [
        "### 2. Download and Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2b90HnObMkm"
      },
      "source": [
        "# download IMDB dataset\n",
        "!wget \"https://raw.githubusercontent.com/javaidnabi31/Word-Embeddding-Sentiment-Classification/master/movie_data.csv\" -O \"movie_data.csv\"  \n",
        "\n",
        "# list files in current directory\n",
        "!ls -lah  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v-9xJvhbb9c"
      },
      "source": [
        "# the path to the IMDB dataset\n",
        "dataset_path = 'movie_data.csv'  \n",
        "\n",
        "# read file (dataset) into our program using pandas\n",
        "data = pd.read_csv(dataset_path) \n",
        "\n",
        "# display first 5 rows\n",
        "data.head()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sAgqwiZbzuU"
      },
      "source": [
        "### 3. Clean Text\n",
        "\n",
        "Define the `clean_review` function to apply on the dataset reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eyrg00Ycb08M"
      },
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "english_stopwords = stopwords.words('english')\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def clean_review(text):\n",
        "  # convert to lower case\n",
        "  text = text.lower()\n",
        "\n",
        "  # remove none alphabetic characters\n",
        "  text = re.sub(r'[^a-z]', ' ', text)\n",
        "\n",
        "  # stem words \n",
        "  # split into words\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # stemming of words\n",
        "  stemmed = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "  text = ' '.join(stemmed)\n",
        "\n",
        "  # remove stopwords\n",
        "  text = ' '.join([word for word in text.split() if word not in english_stopwords])\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "# apply to all dataset\n",
        "data['clean_review'] = data['review'].apply(clean_review)\n",
        "data.head() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pSqLwbYcNqu"
      },
      "source": [
        "### 4. Split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZlGI1CScPLu"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "X = data['clean_review'].values\n",
        "y = data['sentiment'].values\n",
        "\n",
        "# Split data into 50% training & 50% test\n",
        "# let's all use a random state of 42 for example to ensure having the same split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjYirO0ucXUw"
      },
      "source": [
        "### 5. Feature Extraction with Word Embeddings\n",
        "\n",
        "Instead of going with Bag-of-Words for feature extraction, we are using Keras'  `Tokenizer()` class to prepare the data for the `Embedding` layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuS3-0_3cfUP"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# define your tokenizer (with num_words=10000)\n",
        "tokenizer_obj = _______\n",
        "\n",
        "# assign an index (number) to each word using fit_on_texts function  \n",
        "tokenizer_obj._______\n",
        "\n",
        "# will be used later to pad sequences\n",
        "max_length = 120\n",
        "\n",
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer_obj.word_index) + 1\n",
        "\n",
        "# transform each text to a sequence of integers (to be used later in embeddings layer)\n",
        "X_train_tokens =  tokenizer_obj._______\n",
        "X_test_tokens = _______\n",
        "\n",
        "# apply post-padding to the sequences\n",
        "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ67srOxdbu8"
      },
      "source": [
        "x_train[0], X_train_pad[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yqg6oLbyeMzM"
      },
      "source": [
        "## Recurrent Neural Network\n",
        "\n",
        "Now it's time to build the deep RNN network that will model the data. The network has to start with an `Embedding` layer, then we add one or multiple Recurrent layers and finally finish with a couple of Dense layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8GdbHJ2T_QO"
      },
      "source": [
        "### Building and Training the RNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqAkP-UodqOE"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "embedding_dim = 300\n",
        "\n",
        "# FILL BLANKS\n",
        "# build the neural network\n",
        "\n",
        "\n",
        "# compile model: assign loss & optimizer\n",
        "model.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlKvxbu8fpMK"
      },
      "source": [
        "# train model\n",
        "model.fit(X_train_pad, y_train, batch_size=32, epochs=5, validation_data=(X_test_pad, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkXHWbrCg0PD"
      },
      "source": [
        "### Plot training details\n",
        "\n",
        "We visualize the training parameters to have a better understanding of the model's convergence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqZrCxAGfxp_"
      },
      "source": [
        "def plot_accuracy_and_loss(model):\n",
        "    epochs = model.history.params['epochs']\n",
        "    epochs = range(epochs)\n",
        "    val_loss = model.history.history['val_loss']\n",
        "    val_accuracy = model.history.history['val_accuracy']\n",
        "    training_loss = model.history.history['loss']\n",
        "    training_accuracy = model.history.history['accuracy']\n",
        "\n",
        "    plt.plot(epochs, val_loss, 'r', label='test')\n",
        "    plt.plot(epochs, training_loss, 'b', label='training')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(epochs, val_accuracy, 'r', label='test')\n",
        "    plt.plot(epochs, training_accuracy, 'b', label='training')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_accuracy_and_loss(model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}